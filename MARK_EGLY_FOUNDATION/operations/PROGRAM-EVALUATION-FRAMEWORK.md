# Program Evaluation Framework

**Document 14 of 37: Mark Egly Foundation Operations Manual**
**Topic 4: Quality Assurance (Document 1 of 4)**
**Version:** 1.0
**Last Updated:** November 2025
**Document Owner:** Executive Director & Chief Operating Officer
**Review Cycle:** Annual (with quarterly progress reviews)

---

## Table of Contents

1. [Executive Summary](#executive-summary)
2. [Evaluation Philosophy](#evaluation-philosophy)
3. [Foundation Theory of Change](#foundation-theory-of-change)
4. [Logic Model Framework](#logic-model-framework)
5. [Key Performance Indicators (KPIs)](#key-performance-indicators-kpis)
6. [Program-Specific Evaluation Plans](#program-specific-evaluation-plans)
7. [Data Collection Methods](#data-collection-methods)
8. [Evaluation Timeline and Cycles](#evaluation-timeline-and-cycles)
9. [Stakeholder Feedback Systems](#stakeholder-feedback-systems)
10. [Data Analysis and Reporting](#data-analysis-and-reporting)
11. [Quality Improvement Process](#quality-improvement-process)
12. [External Evaluation](#external-evaluation)
13. [Evaluation Budget and Resources](#evaluation-budget-and-resources)
14. [Evaluation Ethics and Privacy](#evaluation-ethics-and-privacy)
15. [Tools and Technology](#tools-and-technology)
16. [Continuous Learning Culture](#continuous-learning-culture)
17. [Conclusion](#conclusion)

---

## Executive Summary

The Mark Egly Foundation's Program Evaluation Framework establishes systematic processes for measuring impact, learning from outcomes, and continuously improving programs serving the Alpha-1 antitrypsin deficiency community. This framework ensures that Foundation resources are deployed effectively, stakeholder needs are met, and progress toward mission goals is rigorously assessed.

**Evaluation Principles:**

1. **Mission-Driven**: Evaluation focuses on patient outcomes, research advancement, and community impact
2. **Learning-Oriented**: Evaluation serves continuous improvement, not just accountability
3. **Stakeholder-Inclusive**: Patients, researchers, providers, and donors inform evaluation design
4. **Rigorous Yet Practical**: Methods are scientifically sound but feasible for small nonprofit
5. **Transparent**: Results shared publicly (successes and challenges) to benefit broader field
6. **Action-Focused**: Evaluation findings drive program adjustments and strategic decisions

**Core Evaluation Questions:**

1. **Impact**: Are we improving outcomes for Alpha-1 patients and advancing research?
2. **Reach**: Are we serving our target populations effectively?
3. **Quality**: Are programs delivered with excellence and satisfaction?
4. **Efficiency**: Are we maximizing impact per dollar invested?
5. **Sustainability**: Are program impacts lasting beyond Foundation intervention?

**Evaluation Maturity Roadmap:**

- **Year 1**: Basic output tracking (activities, participants, publications)
- **Year 2**: Outcome measurement (short-term behavior/knowledge changes)
- **Year 3-5**: Impact assessment (long-term health, research, community outcomes)
- **Year 5+**: Longitudinal studies, comparative effectiveness, external validation

---

## Evaluation Philosophy

### Learning vs. Accountability

**Dual Purpose of Evaluation:**

1. **Accountability**: Demonstrate to funders, Board, and community that Foundation is achieving mission
2. **Learning**: Understand what works, what doesn't, and how to improve

**Foundation's Emphasis**: 70% learning, 30% accountability

- Evaluation findings used primarily to strengthen programs
- "Failed experiments" celebrated as learning opportunities
- Staff encouraged to surface challenges without fear of penalty

### Developmental Evaluation Approach

**Definition**: Evaluation that supports program innovation and adaptation in real-time.

**When Used**: New programs in first 2 years; programs operating in rapidly changing environments (e.g., emerging research areas, novel interventions).

**Characteristics:**

- Frequent data collection and feedback loops (monthly or quarterly)
- Evaluator embedded in program design discussions
- Rapid-cycle testing of program adjustments
- Emphasis on emergent findings and adaptation

**Example**: Foundation launches new telehealth genetic counseling program. Developmental evaluation includes:

- Monthly feedback sessions with counselors and patients
- Real-time tracking of uptake, completion, satisfaction
- Quarterly program adjustments based on findings (e.g., adjusting session length, adding interpretation services)

### Utilization-Focused Evaluation

**Core Principle**: Evaluation is only valuable if findings are used.

**Foundation Practices:**

1. **Involve Intended Users**: ED, COO, Program Managers, Board participate in designing evaluation questions
2. **Focus on Actionable Insights**: Avoid collecting data that won't inform decisions
3. **Timely Reporting**: Quarterly dashboards, not just annual reports
4. **Accessible Formats**: Infographics, executive summaries, not just dense reports
5. **Decision Forums**: Quarterly "Evaluation to Action" meetings where findings drive program decisions

---

## Foundation Theory of Change

### Overall Theory of Change

**Mission**: To improve outcomes for individuals with Alpha-1 antitrypsin deficiency through research, awareness, and support.

**Theory of Change Logic:**

```
IF we...
├─ Increase awareness of Alpha-1 among patients and providers
├─ Expand genetic testing and diagnosis
├─ Build comprehensive patient registry
├─ Fund high-quality research
├─ Support patient access to care and resources
└─ Advocate for policy improvements

THEN...
├─ More individuals diagnosed early (before irreversible organ damage)
├─ Patients receive appropriate treatment and management
├─ Research accelerates discovery of better diagnostics and treatments
├─ Health outcomes improve (slower disease progression, better quality of life)
└─ Alpha-1 community is empowered and supported

RESULTING IN...
└─ Long-term vision: Alpha-1 becomes routinely screened, promptly diagnosed,
   effectively treated, and ultimately cured or prevented
```

### Assumptions Underlying Theory of Change

**Critical Assumptions to Test Through Evaluation:**

1. **Awareness drives testing**: Provider/patient awareness of Alpha-1 leads to increased genetic testing
   - _Evaluation Test_: Track correlation between awareness campaigns and regional testing rates

2. **Registry enables research**: Comprehensive registry accelerates research by facilitating patient recruitment and data analysis
   - _Evaluation Test_: Survey researchers on registry utility; track studies using registry data

3. **Early diagnosis improves outcomes**: Patients diagnosed early have better health trajectories than those diagnosed late
   - _Evaluation Test_: Compare outcomes of early-stage vs. late-stage patients in registry (control for disease severity)

4. **Research funding yields discoveries**: Foundation-funded research produces meaningful scientific advances
   - _Evaluation Test_: Track publications, citations, clinical trial initiations from funded grants

5. **Patient support improves adherence**: Support programs increase treatment adherence and care engagement
   - _Evaluation Test_: Survey patients before/after support program participation; track adherence metrics

**Documenting Assumption Tests**: Annual "Theory of Change Review" assesses evidence for/against each assumption; theory revised as needed.

---

## Logic Model Framework

### Foundation-Wide Logic Model

| **Component**                         | **Description**                                      | **Examples**                                                                                                                 |
| ------------------------------------- | ---------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------- |
| **Inputs**                            | Resources invested                                   | Staff time, funding, partnerships, volunteers, data                                                                          |
| **Activities**                        | What Foundation does                                 | Awareness campaigns, research grants, registry enrollment, patient workshops, advocacy                                       |
| **Outputs**                           | Direct products of activities                        | # awareness events, # grant recipients, # registry participants, # resources distributed                                     |
| **Short-Term Outcomes** (1-2 years)   | Immediate changes in knowledge, attitudes, behaviors | Increased Alpha-1 knowledge among providers, more patients seeking testing, registry growth                                  |
| **Intermediate Outcomes** (3-5 years) | Sustained behavior changes, system changes           | Higher diagnosis rates, research publications, treatment guideline updates, policy changes                                   |
| **Long-Term Impact** (5-10+ years)    | Ultimate mission achievement                         | Reduced disease burden, earlier diagnosis (avg. age drops from 45 to 35), new treatments available, improved quality of life |

### Program-Specific Logic Models

**Each Major Program Develops Logic Model:**

1. **Awareness & Education Program**
2. **Patient Registry**
3. **Research Grants Program**
4. **Patient Support Services**
5. **Provider Education**
6. **Advocacy & Policy**

**Example: Awareness & Education Program Logic Model**

| **Inputs**                                                          | **Activities**                                                                                             | **Outputs**                                                                                                        | **Short-Term Outcomes**                                                                                                              | **Intermediate Outcomes**                                                                                              | **Long-Term Impact**                                                                                                    |
| ------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------- |
| $150K budget<br>1.5 FTE staff<br>Social media<br>Physician partners | Social media campaigns<br>Conference exhibits<br>Provider webinars<br>Patient stories<br>Website resources | 500K social impressions<br>50 conference presentations<br>10 webinars<br>20 patient stories<br>100K website visits | 10K providers aware of Alpha-1<br>5K patients learn about testing<br>1K providers access education<br>500 patients share experiences | 2K additional tests ordered<br>300 new diagnoses<br>Providers screen COPD patients routinely<br>Patient advocacy grows | Earlier diagnosis (avg. age decreases)<br>Reduced diagnostic odyssey<br>Alpha-1 becomes "household name" in pulmonology |

---

## Key Performance Indicators (KPIs)

### Foundation-Level KPIs

**Mission Impact KPIs (Primary):**

| **KPI**                                                              | **Baseline**                | **Year 1 Target** | **Year 3 Target** | **Year 5 Target** | **Data Source**                                             |
| -------------------------------------------------------------------- | --------------------------- | ----------------- | ----------------- | ----------------- | ----------------------------------------------------------- |
| **# Individuals Diagnosed** (attributable to Foundation)             | 0                           | 150               | 600               | 1,500             | Registry intake surveys, provider referrals                 |
| **Average Age at Diagnosis** (among Foundation-reached patients)     | 45 years (national avg.)    | 44 years          | 42 years          | 40 years          | Registry data                                               |
| **# Active Patient Registry Participants**                           | 0                           | 500               | 2,000             | 5,000             | Registry database                                           |
| **# Peer-Reviewed Publications** (from Foundation-funded research)   | 0                           | 3                 | 15                | 40                | Grant recipient reports, PubMed                             |
| **Patient Quality of Life Score** (avg. among registry participants) | [Establish baseline Year 1] | Baseline + 5%     | Baseline + 10%    | Baseline + 15%    | Annual registry survey (SF-36 or Alpha-1-specific QOL tool) |

**Operational KPIs (Secondary):**

| **KPI**                          | **Year 1 Target** | **Year 3 Target** | **Year 5 Target** | **Data Source**             |
| -------------------------------- | ----------------- | ----------------- | ----------------- | --------------------------- |
| **Total Revenue**                | $750K             | $2.5M             | $5M               | Financial records           |
| **Program Expense Ratio**        | 70%               | 75%               | 80%               | Financial records           |
| **Donor Retention Rate**         | 60%               | 70%               | 75%               | CRM                         |
| **Website Unique Visitors**      | 25K               | 100K              | 250K              | Google Analytics            |
| **Social Media Engagement Rate** | 2%                | 4%                | 6%                | Social media analytics      |
| **Provider Partners/Advocates**  | 25                | 100               | 250               | CRM, partnership database   |
| **Volunteer Hours Contributed**  | 500               | 2,000             | 5,000             | Volunteer management system |

### Program-Specific KPIs

**Patient Registry KPIs:**

- Enrollment rate (new participants/month)
- Completion rate (% completing annual survey)
- Data quality score (% fields complete, accuracy)
- Researcher utilization (# research projects using registry data)
- Patient satisfaction with registry experience (annual survey)

**Research Grants Program KPIs:**

- # grant applications received
- # grants awarded
- Total $ distributed
- # publications per grant (avg. 2+ years post-funding)
- Citation impact (avg. citations per publication)
- # clinical trials initiated using Foundation-funded preliminary data

**Awareness Campaign KPIs:**

- Reach (# individuals exposed to campaign)
- Engagement (click-through rates, video views, social shares)
- Knowledge increase (pre/post campaign surveys)
- Behavior change (testing inquiries, website visits to "Get Tested" page)
- Provider awareness (% pulmonologists aware of Alpha-1, tracked via periodic surveys)

---

## Program-Specific Evaluation Plans

### Evaluation Plan Template

**Each Program Develops Annual Evaluation Plan Including:**

1. **Program Description**: Goals, target population, activities
2. **Evaluation Questions**: 3-5 key questions (impact, reach, quality, efficiency)
3. **Indicators**: Specific measures for each question
4. **Data Collection Methods**: Surveys, interviews, administrative data, observations
5. **Timeline**: When data collected
6. **Responsible Parties**: Who collects, analyzes, reports
7. **Budget**: Costs for evaluation activities
8. **Use Plan**: How findings will inform decisions

### Example: Patient Registry Evaluation Plan (Year 2)

**Program Description:**

Build and maintain comprehensive patient registry to accelerate research, improve care, and measure patient outcomes.

**Evaluation Questions:**

1. **Impact**: Does registry facilitate high-quality research?
2. **Reach**: Are we enrolling diverse, representative patient population?
3. **Quality**: Is registry data accurate, complete, and useful?
4. **Efficiency**: Are enrollment and maintenance costs reasonable?
5. **Satisfaction**: Are participants and researchers satisfied with registry?

**Indicators & Methods:**

| **Question**     | **Indicators**                                                                 | **Methods**                                                             | **Timeline**                        |
| ---------------- | ------------------------------------------------------------------------------ | ----------------------------------------------------------------------- | ----------------------------------- |
| **Impact**       | # research studies using registry; # publications; researcher-reported value   | Annual researcher survey; PubMed tracking                               | Annually (Q4)                       |
| **Reach**        | Enrollment rate; demographic representativeness (vs. known Alpha-1 population) | Registry database analysis; comparison to national Alpha-1 demographics | Quarterly                           |
| **Quality**      | % fields complete; data accuracy (validation sample); duplicate rate           | Automated data quality reports; annual validation audit                 | Monthly (automated); Annual (audit) |
| **Efficiency**   | Cost per enrolled participant; cost per complete annual survey                 | Financial analysis                                                      | Quarterly                           |
| **Satisfaction** | Participant satisfaction (5-point scale); researcher satisfaction              | Annual participant survey; researcher feedback                          | Annually (Q1)                       |

**Use Plan:**

- **Q1**: Use satisfaction data to improve participant experience (e.g., simplify survey, improve communication)
- **Q2**: Use reach data to target underrepresented groups for enrollment
- **Q3**: Use efficiency data to optimize enrollment workflows
- **Q4**: Use impact data to demonstrate value to funders and Board; inform Year 3 priorities

**Budget:** $8,000 (survey platform $1,000; data validation audit $5,000; analyst time $2,000)

---

## Data Collection Methods

### Quantitative Methods

**Administrative Data:**

- **Source**: Internal databases (CRM, registry, financial system, website analytics)
- **Measures**: Outputs (# events, participants, publications), costs, reach
- **Frequency**: Continuous/automated reporting
- **Strengths**: Low burden, objective, comprehensive
- **Limitations**: Limited to tracked activities; doesn't capture outcomes

**Surveys:**

- **Purpose**: Measure knowledge, attitudes, behaviors, satisfaction, outcomes
- **Platforms**: Qualtrics, SurveyMonkey, Google Forms (HIPAA-compliant options for patient data)
- **Types**:
  - **Pre/Post Surveys**: Measure change from program participation (e.g., provider knowledge before/after webinar)
  - **Longitudinal Surveys**: Track outcomes over time (e.g., annual patient registry survey)
  - **One-Time Surveys**: Assess satisfaction, gather feedback
- **Best Practices**:
  - Keep brief (5-10 minutes; <20 questions)
  - Use validated scales when possible (e.g., SF-36 for quality of life)
  - Pilot test with small group before full launch
  - Offer incentives for completion ($10-$25 gift cards; raffle entry)
  - Send reminders (day 3, day 7, day 14)
- **Expected Response Rates**:
  - Registry participants: 60-70% (high engagement)
  - Event participants: 30-50%
  - Provider surveys: 20-30%

**Outcome Measurement Scales:**

Use validated instruments when available:

- **Quality of Life**: SF-36 (Short Form Health Survey) or Alpha-1-specific QOL scale (if developed)
- **Patient Activation**: Patient Activation Measure (PAM) - measures patient engagement in care
- **Depression/Anxiety**: PHQ-9 (depression), GAD-7 (anxiety)
- **Disease Knowledge**: Develop Alpha-1-specific knowledge quiz (validate through expert review)

### Qualitative Methods

**Interviews:**

- **Purpose**: Deep understanding of experiences, motivations, barriers
- **Types**:
  - **Key Informant Interviews**: Experts, partners, stakeholders (e.g., leading Alpha-1 researchers, patient advocates)
  - **Patient Interviews**: Understand patient journeys, needs, program impact
  - **Provider Interviews**: Understand barriers to testing, treatment practices
- **Sample Size**: 10-20 interviews typically sufficient for thematic saturation
- **Duration**: 30-60 minutes
- **Format**: Phone, video (Zoom), or in-person
- **Compensation**: $50-$100 honorarium for patient/provider time

**Focus Groups:**

- **Purpose**: Group discussion to surface diverse perspectives, generate ideas
- **Sample Uses**:
  - Patient focus groups to design new support program
  - Provider focus groups to understand testing barriers
  - Donor focus groups to assess fundraising messaging
- **Size**: 6-10 participants
- **Duration**: 90 minutes
- **Facilitation**: Use trained facilitator (external preferred to encourage candor)

**Case Studies:**

- **Purpose**: In-depth exploration of specific patient stories, program examples, research collaborations
- **Methods**: Document review, interviews, observation
- **Use**: Illustrate impact for funders, demonstrate proof-of-concept for new programs

### Mixed Methods Approach

**Foundation Standard**: Combine quantitative (surveys, administrative data) with qualitative (interviews) for comprehensive understanding.

**Example**: Awareness Campaign Evaluation

- **Quantitative**: Track website visits, social media engagement, testing inquiries (metrics)
- **Quantitative**: Survey 200 providers on Alpha-1 knowledge and testing practices (pre/post campaign)
- **Qualitative**: Interview 15 patients who were diagnosed after seeing campaign (understand decision pathway)

Result: Numbers show reach and behavior change; interviews explain _why_ campaign worked and reveal areas for improvement.

---

## Evaluation Timeline and Cycles

### Annual Evaluation Calendar

| **Month**     | **Evaluation Activities**                                                                      |
| ------------- | ---------------------------------------------------------------------------------------------- |
| **January**   | Deploy annual patient registry survey; compile Year prior program reports                      |
| **February**  | Analyze Q4 and Year prior data; draft Annual Impact Report                                     |
| **March**     | Board presentation: Year prior results; external evaluation planning (if conducted)            |
| **April**     | Q1 data review; program adjustments; launch new evaluation projects                            |
| **May**       | Deploy grant recipient outcome surveys (2 years post-funding)                                  |
| **June**      | Stakeholder feedback collection (patients, researchers, providers)                             |
| **July**      | Q2 data review; mid-year evaluation report                                                     |
| **August**    | Strategic planning integration (evaluation findings inform Year +1 priorities)                 |
| **September** | Focus groups or interviews (rotating topics annually)                                          |
| **October**   | Q3 data review; draft Year +1 evaluation plans for each program                                |
| **November**  | Budget development (Year +1 evaluation budgets); external evaluator engagement (if applicable) |
| **December**  | Program-specific evaluation reports due; Q4 data collection                                    |

### Quarterly Review Cycles

**Quarterly "Evaluation to Action" Meetings** (Last week of Q1, Q2, Q3, Q4):

**Participants**: ED, COO, Program Managers, Board Evaluation Committee Chair

**Agenda (2 hours):**

1. **Dashboard Review (30 min)**: Review KPI dashboard; identify trends, concerns, successes
2. **Deep Dive (45 min)**: One program presents detailed evaluation findings (rotate programs each quarter)
3. **Action Planning (30 min)**: Decide program adjustments based on findings
4. **Learning Reflection (15 min)**: What did we learn? What surprised us? What should we investigate further?

**Outputs:**

- Updated KPI dashboard
- Program improvement action items (assigned owners, deadlines)
- Evaluation questions to explore in next quarter

### Real-Time Monitoring

**Monthly Automated Reports** (auto-generated from databases):

- Registry enrollment and survey completion
- Website traffic and engagement
- Social media metrics
- Grant application and award tracking
- Fundraising progress

**Red Flag Alerts**: If metrics fall >20% below target, automatic alert to ED/COO for immediate attention.

---

## Stakeholder Feedback Systems

### Patient Advisory Board

**Purpose**: Ensure patient voice informs evaluation priorities and program design.

**Composition**: 8-12 patients with Alpha-1; diverse by disease stage, demographics, geography.

**Meeting Frequency**: Quarterly (virtual or hybrid).

**Evaluation Role:**

- Review draft evaluation plans; suggest additional questions
- Interpret evaluation findings from patient perspective
- Provide ongoing feedback on program experiences
- Participate in focus groups or interviews as needed

### Researcher Advisory Panel

**Purpose**: Ensure research program evaluation is rigorous and relevant.

**Composition**: 5-8 Alpha-1 researchers from academic institutions.

**Meeting Frequency**: Semi-annually.

**Evaluation Role:**

- Review research grant evaluation methodology
- Assess registry data quality and utility
- Provide input on measuring research impact

### Provider Network Feedback

**Mechanism**: Annual online survey of provider partners (pulmonologists, genetic counselors, primary care physicians).

**Survey Topics:**

- Usefulness of Foundation resources (educational materials, patient referrals)
- Barriers to Alpha-1 testing and diagnosis
- Suggestions for new programs or resources
- Satisfaction with Foundation responsiveness

**Response Goal**: 40% response rate (80+ providers if network = 200).

**Incentive**: $25 gift card; entry to raffle for CME conference registration.

### Donor Feedback

**Annual Donor Survey**: Sample of major donors ($1,000+) and random sample of smaller donors.

**Survey Topics:**

- Satisfaction with communication and stewardship
- Perceived Foundation impact
- Interest in specific programs
- Likelihood to continue giving (Net Promoter Score)

**Use**: Improve donor relations; identify impact messaging that resonates.

---

## Data Analysis and Reporting

### Data Analysis Approach

**Descriptive Statistics**: Frequencies, percentages, means, medians for most program data (e.g., "70% of webinar participants increased knowledge score by ≥10%").

**Trend Analysis**: Track indicators over time; identify upward/downward trends (e.g., registry enrollment rate).

**Comparative Analysis**: Compare groups when relevant (e.g., outcomes of patients diagnosed early vs. late; satisfaction by program type).

**Statistical Significance Testing**: Use when sample sizes adequate (n>30) and comparison meaningful (e.g., pre/post survey scores, intervention vs. control groups).

**Qualitative Coding**: Thematic analysis of interviews and open-ended survey responses using NVivo or Dedoose software.

### Analysis Staffing

**Internal Capacity:**

- **Year 1**: ED and COO conduct basic analysis (Excel, Google Analytics)
- **Year 2-3**: Hire part-time Data Analyst or engage evaluation consultant ($10K-$20K/year)
- **Year 4+**: Consider full-time Program Evaluation Manager (0.5-1.0 FTE)

**External Support:**

- Partner with local university MPH program (graduate student practicum projects)
- Engage evaluation consultant for complex analyses ($5K-$15K per project)

### Reporting Formats

**1. KPI Dashboard (Real-Time, Automated):**

- Visual dashboard (Tableau, Google Data Studio, or simple Excel)
- Updated automatically from databases
- Accessible to ED, COO, Board
- Color-coded indicators (green = on track; yellow = caution; red = off track)

**2. Quarterly Program Reports (Template):**

Each program completes 2-3 page report:

- **Outputs This Quarter**: Activities and outputs
- **Progress Toward Outcomes**: KPI status, short-term outcome data
- **Successes**: What worked well
- **Challenges**: What didn't work; barriers encountered
- **Adjustments**: Program changes made or planned based on learning
- **Next Quarter Priorities**

**3. Annual Impact Report (Public):**

20-30 page report for external audiences (donors, partners, public):

- **Executive Summary**: Key accomplishments, impacts, stories
- **By the Numbers**: Foundation-wide KPIs
- **Program Highlights**: Each program's key outcomes
- **Patient Stories**: 3-5 case studies illustrating impact
- **Research Advances**: Publications, trials, discoveries funded
- **Financials**: Revenue, expenses, program allocation
- **Looking Ahead**: Year +1 priorities

Distributed as PDF, web version, and printed copies for major donors.

**4. Board Evaluation Briefings (Quarterly):**

Concise slide deck (10-15 slides):

- KPI dashboard snapshot
- Deep dive on one program
- Evaluation findings and program adjustments
- Key decisions needed from Board

Presented by ED/COO at Board meetings.

**5. External Evaluation Reports (Periodic):**

Formal reports by external evaluators (every 3-5 years):

- Comprehensive impact assessment
- Comparative analysis (Foundation vs. national trends)
- Recommendations for strategic direction
- Validation of internal evaluation findings

Submitted to Board; executive summary published.

---

## Quality Improvement Process

### Plan-Do-Study-Act (PDSA) Cycles

**Framework**: Rapid-cycle testing for program improvements.

**Process:**

1. **Plan**: Identify problem/opportunity; design small-scale change to test; predict outcome
2. **Do**: Implement change on small scale; document observations
3. **Study**: Analyze data; compare outcome to prediction; summarize learnings
4. **Act**: If successful, implement at full scale; if not, modify and repeat cycle

**Example PDSA Cycle: Improving Registry Survey Completion**

- **Plan**: Problem: Registry survey completion rate is 50%, target 70%. Change to Test: Shorten survey from 45 min to 25 min by removing low-priority questions. Prediction: Completion rate will increase to 65%.

- **Do**: Deploy shortened survey to next 100 participants (Month 1).

- **Study**: Completion rate increased to 67% among test group. Feedback: "Much more manageable"; "Still long but doable."

- **Act**: Implement shortened survey for all participants. Continue monitoring. Next PDSA: Test $15 gift card incentive to push completion to 75%.

**PDSA Documentation**: Maintain PDSA log (simple spreadsheet) documenting each cycle's plan, data, outcome, and decision.

### Continuous Quality Improvement (CQI) Committee

**Purpose**: Drive ongoing program improvement using evaluation data.

**Composition**: COO (chair), Program Managers, Data Analyst, patient representative.

**Meeting Frequency**: Monthly (1 hour).

**Agenda:**

1. Review evaluation data from past month
2. Identify quality improvement opportunities (program issues, bottlenecks, low-performing metrics)
3. Design PDSA cycles to test improvements
4. Track active PDSA cycles; review results
5. Celebrate successes; document lessons learned

### Root Cause Analysis

**When Used**: Significant program failure or persistent problem (e.g., low provider engagement, grant recipients not publishing results).

**Method**: "5 Whys" technique

**Example: Grant Recipients Not Publishing Results**

- **Problem**: 50% of grant recipients have not published within 2 years of grant end.
- **Why #1**: Why haven't they published? → "Too busy with clinical duties."
- **Why #2**: Why too busy? → "Grants too small to buy protected research time."
- **Why #3**: Why grants too small? → "Foundation prioritizes funding many small grants over fewer large grants."
- **Why #4**: Why that priority? → "Assumption that more grants = more research activity."
- **Why #5**: Why that assumption? → "No data on grant size vs. publication success."

**Root Cause**: Grant size strategy not evidence-based.

**Solution**: Test larger grants ($75K-$100K with salary support) vs. current small grants ($25K-$40K); track publication outcomes; adjust strategy based on results.

---

## External Evaluation

### When to Conduct External Evaluation

**Recommended Timing:**

- **Year 3**: Mid-point evaluation (formative focus; strengthen programs)
- **Year 5**: Summative evaluation (assess overall impact; inform strategic planning)
- **Every 5 Years**: Periodic comprehensive evaluation

**Purpose:**

- Independent validation of Foundation's impact claims
- Credibility with funders and stakeholders
- Expertise Foundation lacks internally (e.g., quasi-experimental designs, advanced statistical methods)
- Fresh perspective; identify blind spots

### Selecting External Evaluator

**Criteria:**

1. **Expertise**: Evaluation of health/rare disease nonprofits; familiarity with Alpha-1 or respiratory disease (preferred)
2. **Methods**: Mixed methods capability; rigorous quantitative and qualitative skills
3. **Nonprofit Experience**: Understanding of resource constraints; practical recommendations
4. **Compatibility**: Evaluator committed to utilization-focused approach; collaborative style
5. **Cost**: Reasonable fees; efficient (minimize Foundation staff burden)

**Potential Evaluators:**

- University-based evaluation centers (MPH/public health programs)
- Nonprofit evaluation consultancies (e.g., Mathematica Policy Research, NORC, smaller regional firms)
- Independent evaluation consultants

**Procurement Process:**

1. Issue RFP (Request for Proposals) with evaluation questions, scope, budget
2. Review 3-5 proposals
3. Conduct finalist interviews (assess fit, approach)
4. Check references
5. Negotiate contract

**Budget**: $25,000-$75,000 for comprehensive external evaluation (depending on scope, evaluator seniority).

### Scope of External Evaluation

**Typical Components:**

1. **Evaluation Design and Plan**: Refine questions, methods, timeline
2. **Data Collection**: Surveys, interviews, focus groups (external evaluator leads or supports)
3. **Data Analysis**: Quantitative and qualitative analysis
4. **Reporting**: Written report, presentation to Board and stakeholders
5. **Utilization Support**: Facilitate sessions to translate findings into action

**Timeline**: 6-12 months from contract to final report.

---

## Evaluation Budget and Resources

### Evaluation Budget (Annual)

**Year 1 (Startup, Basic Evaluation):**

| **Category**               | **Items**                                     | **Cost**          |
| -------------------------- | --------------------------------------------- | ----------------- |
| **Survey Platform**        | Qualtrics or SurveyMonkey (HIPAA-compliant)   | $1,500            |
| **Data Analysis Software** | Excel, Google Analytics (free); Tableau trial | $0                |
| **Staff Time**             | ED/COO evaluation activities (10% FTE)        | $15,000 (in-kind) |
| **Survey Incentives**      | Gift cards for registry, stakeholder surveys  | $2,000            |
| **Evaluation Consultant**  | Support for evaluation plan design            | $5,000            |
| **Training**               | ED/COO attend evaluation training workshop    | $1,500            |
| **TOTAL**                  |                                               | **$25,000**       |

**Years 2-3 (Developing Capacity):**

| **Category**               | **Items**                            | **Cost**              |
| -------------------------- | ------------------------------------ | --------------------- |
| **Survey Platform**        | Qualtrics or SurveyMonkey            | $2,000                |
| **Data Analysis Software** | Tableau, NVivo (qualitative), SPSS/R | $3,000                |
| **Part-Time Data Analyst** | 0.25 FTE (contract or staff)         | $20,000               |
| **Survey Incentives**      | Gift cards, raffles                  | $3,000                |
| **Evaluation Consultant**  | Focus groups, interview support      | $10,000               |
| **External Evaluation**    | Year 3: Mid-point evaluation         | $40,000 (Year 3 only) |
| **Training**               | Staff evaluation skills development  | $2,000                |
| **TOTAL (Year 2)**         |                                      | **$40,000**           |
| **TOTAL (Year 3)**         |                                      | **$80,000**           |

**Years 4-5 (Mature Evaluation System):**

| **Category**               | **Items**                                   | **Cost**              |
| -------------------------- | ------------------------------------------- | --------------------- |
| **Survey Platform**        | Qualtrics or SurveyMonkey                   | $2,500                |
| **Data Analysis Software** | Tableau, NVivo, SPSS/R; licenses            | $4,000                |
| **Evaluation Staff**       | 0.5 FTE Program Evaluation Manager          | $45,000               |
| **Survey Incentives**      | Gift cards, raffles                         | $5,000                |
| **Evaluation Consultant**  | Specialized projects (longitudinal studies) | $15,000               |
| **External Evaluation**    | Year 5: Summative evaluation                | $60,000 (Year 5 only) |
| **Conferences/Training**   | American Evaluation Association conference  | $3,000                |
| **TOTAL (Year 4)**         |                                             | **$74,500**           |
| **TOTAL (Year 5)**         |                                             | **$134,500**          |

**Evaluation as % of Budget:**

- **Target**: 5-8% of total Foundation budget
- **Year 1**: 3-4% (basic evaluation)
- **Years 2-5**: 4-6% (growing capacity)
- **Year 5** (with external evaluation): 8-10%

### Evaluation Staffing Model

**Year 1**: ED + COO (combined 0.2 FTE on evaluation)

**Year 2-3**: Add part-time Data Analyst (0.25 FTE, $20K-$25K), increases to 0.5 FTE Year 3

**Year 4-5**: Hire Program Evaluation Manager (0.5-1.0 FTE, $50K-$90K salary + benefits)

**Position Description: Program Evaluation Manager (0.5 FTE, Years 4-5)**

- Design and implement evaluation plans for all programs
- Manage data collection, analysis, and reporting
- Maintain KPI dashboards and quarterly reports
- Coordinate with external evaluators
- Build evaluation capacity among program staff
- Present findings to Board and stakeholders

**Qualifications**: MPH, MPA, or MA in Evaluation/Social Sciences; 3-5 years evaluation experience; mixed methods expertise; health/nonprofit background preferred.

---

## Evaluation Ethics and Privacy

### Ethical Principles

1. **Do No Harm**: Evaluation does not burden participants excessively; protects vulnerable populations
2. **Informed Consent**: Participants understand evaluation purpose, how data will be used, voluntary nature
3. **Confidentiality**: Individual responses kept confidential; only aggregate data reported
4. **Respect**: Evaluation respectful of participants' time, perspectives, culture
5. **Honesty**: Report findings accurately, including negative results; no cherry-picking data
6. **Utilization**: Ensure evaluation is actually used; don't collect data without purpose

### IRB Considerations

**When IRB Required:**

Research involving human subjects that will be published or presented as generalizable knowledge may require Institutional Review Board (IRB) approval.

**Foundation's Evaluation Activities:**

- **Likely Exempt**: Quality improvement, program evaluation, satisfaction surveys (data used solely for program improvement, not generalizable research)
- **May Require IRB**: Registry data analysis published in peer-reviewed journals; experimental program evaluation designs (randomized trials)

**Foundation Approach:**

- **Year 1-2**: Operate under quality improvement exemption; no IRB
- **Year 3+**: Establish IRB review process if publishing evaluation findings as research
  - **Option 1**: Apply to local university IRB for Foundation evaluation protocol
  - **Option 2**: Contract with independent IRB (e.g., WCG IRB, Advarra) - cost: $2,000-$5,000/year

### Data Privacy and Security

**HIPAA Compliance**: Registry evaluation involving identifiable patient data must comply with HIPAA (see HIPAA Implementation Playbook).

**De-identification**: When possible, analyze de-identified data (removing names, dates of birth, medical record numbers).

**Data Use Agreements**: If sharing evaluation data with external evaluators, execute Data Use Agreement specifying permitted uses, security requirements.

**Data Retention**: Evaluation data retained per Foundation retention policy (typically 7 years); then securely destroyed.

---

## Tools and Technology

### Survey Platforms

**Options:**

1. **Qualtrics** (HIPAA-compliant tier)
   - Cost: $1,500-$3,000/year
   - Features: Advanced logic, validated scales library, strong analytics, integrations
   - Best For: Complex surveys, patient data

2. **SurveyMonkey** (HIPAA-compliant tier)
   - Cost: $1,500-$2,500/year
   - Features: User-friendly, good templates, adequate analytics
   - Best For: General stakeholder surveys, ease of use

3. **REDCap** (Research Electronic Data Capture)
   - Cost: Free (if academic partner provides access) or $500/year (hosted)
   - Features: Designed for research; HIPAA-compliant; excellent for longitudinal surveys
   - Best For: Registry surveys, research-grade data collection

**Recommendation**: Year 1 start with SurveyMonkey; Year 2+ transition to REDCap for registry surveys (more robust) and Qualtrics for stakeholder surveys.

### Data Analysis and Visualization

**Quantitative Analysis:**

- **Excel**: Basic descriptive statistics (free)
- **Google Sheets**: Collaborative analysis (free)
- **Tableau Public**: Data visualization (free); Tableau Desktop ($70/month, powerful)
- **SPSS**: Statistical analysis ($99/month subscription)
- **R/RStudio**: Statistical analysis (free, open source; requires coding skills)

**Qualitative Analysis:**

- **NVivo**: Leading qualitative data analysis software ($1,400/year)
- **Dedoose**: Web-based, affordable ($24/month)
- **MAXQDA**: Powerful mixed-methods tool ($1,200/year)

**Dashboard Platforms:**

- **Google Data Studio**: Free, integrates with Google Analytics, Sheets
- **Tableau**: Industry standard, powerful visuals ($70/user/month)
- **Power BI**: Microsoft tool, good Excel integration ($10/user/month)

**Recommendation**:

- **Year 1**: Excel/Google Sheets + Google Data Studio (all free)
- **Year 2-3**: Add Tableau ($840/year) for dashboards; Dedoose ($288/year) for qualitative
- **Year 4+**: Full toolkit (Tableau, NVivo, SPSS or R)

### Data Management

**Registry Database**: See Program Evaluation Framework for details; registry database is primary data source for patient outcome evaluation.

**CRM Integration**: Pull donor, volunteer, partner data from CRM (e.g., Salesforce, Bloomerang) for operational KPIs.

**Automated Data Pipelines**: As capacity grows, build automated data flows from source systems (registry, CRM, website, social media) into central data warehouse or dashboard platform. Reduces manual data compilation.

---

## Continuous Learning Culture

### Creating Learning-Oriented Environment

**Leadership Modeling:**

- ED and COO regularly share evaluation findings (positive and negative) with staff and Board
- Frame "failures" as learning opportunities
- Celebrate data-driven program improvements

**Staff Engagement in Evaluation:**

- Program staff involved in designing evaluation questions (ensures relevance)
- Staff review evaluation data regularly, not just annually
- Evaluation findings discussed in team meetings; staff propose program adjustments

**Transparent Reporting:**

- Share evaluation findings publicly (Annual Impact Report, website)
- Acknowledge challenges and areas for improvement, not just successes
- Document lessons learned for benefit of broader rare disease nonprofit field

### Learning from the Field

**Peer Learning:**

- Join evaluation communities of practice (e.g., American Evaluation Association, Nonprofit Evaluation Network)
- Attend nonprofit evaluation conferences
- Connect with peer rare disease organizations to share evaluation practices

**Comparative Benchmarking:**

- Compare Foundation KPIs to national Alpha-1 statistics (diagnosis rates, patient outcomes)
- Benchmark against peer rare disease organizations (cost per patient served, fundraising efficiency)
- Participate in nonprofit benchmarking surveys (e.g., GuideStar, Candid)

**Contributing to Field:**

- Publish evaluation findings (rare disease conference presentations, nonprofit journals)
- Share evaluation tools (logic models, survey instruments) with peer organizations
- Co-author papers on evaluation practices in rare disease nonprofits

---

## Conclusion

The Mark Egly Foundation's Program Evaluation Framework ensures that every dollar, every program, and every activity is rigorously assessed for impact and continuously improved. By embracing both accountability and learning, the Foundation positions itself to maximize benefit for the Alpha-1 community and set a standard for evaluation excellence in the rare disease field.

**Key Implementation Steps (First Year):**

1. **Month 1**: Adopt Program Evaluation Framework; assign evaluation responsibilities
2. **Month 2**: Develop Foundation-wide logic model and KPI dashboard
3. **Month 3**: Create program-specific logic models and evaluation plans
4. **Month 4**: Implement data collection systems (surveys, administrative data tracking)
5. **Month 6**: First quarterly Evaluation to Action meeting
6. **Month 9**: Launch Patient Advisory Board; gather stakeholder feedback
7. **Month 12**: Complete first Annual Impact Report; evaluate Year 1 evaluation system effectiveness

**Building Toward Maturity:**

- **Year 1**: Establish foundation (logic models, KPIs, basic data collection)
- **Year 2**: Strengthen capacity (hire data analyst, improve methods, first systematic stakeholder feedback)
- **Year 3**: Demonstrate impact (external evaluation validates findings; publish outcomes)
- **Year 4-5**: Optimize and innovate (advanced analyses, longitudinal studies, comparative effectiveness)

**Ultimate Vision:**

The Foundation becomes a learning organization where:

- Every decision is informed by evidence
- Programs continuously evolve based on rigorous evaluation
- Failures are analyzed and lessons applied
- Successes are documented and scaled
- The Alpha-1 community sees measurable improvements in diagnosis, treatment, quality of life, and research progress

By treating evaluation not as a compliance exercise but as a strategic asset for learning and improvement, the Mark Egly Foundation ensures its work has lasting, measurable impact on the lives of individuals with Alpha-1 antitrypsin deficiency.

---

**Document 14 of 37 Complete**

_This framework should be reviewed annually and updated as Foundation's evaluation capacity matures and new evaluation questions emerge. Evaluation plans should remain flexible and responsive to organizational learning needs._
