# EVALUATION & IMPACT MEASUREMENT

**Rigorous Methods for Demonstrating Health Outcomes**

---

[← Previous: Methodology](./03_METHODOLOGY_IMPLEMENTATION.md) | [Back to Index](./00_GRANT_PROPOSAL_INDEX.md) | [Next: Social Impact →](./05_SOCIAL_IMPACT_EQUITY.md)

**Status**: ✅ Complete
**Word Count**: ~5,500 words
**Last Updated**: November 9, 2025

---

## Executive Summary

This section outlines our comprehensive evaluation framework designed to rigorously measure the health impact and effectiveness of the Uniting Doctors platform. We employ mixed-methods research combining quantitative metrics, qualitative case studies, and validated survey instruments to demonstrate:

1. **Health Outcomes**: Earlier diagnoses, reduced medical errors, improved patient care
2. **Knowledge Impact**: Physician awareness, practice changes, evidence adoption
3. **Platform Success**: User engagement, retention, satisfaction
4. **Cost Effectiveness**: Healthcare savings, return on investment

All evaluation activities will be conducted in partnership with an independent external evaluator to ensure objectivity and academic rigor.

---

## Evaluation Framework Overview

### Logic Model

**Inputs** → **Activities** → **Outputs** → **Outcomes** → **Impact**

**Inputs**:

- Grant funding ($500K-2M)
- Technology infrastructure
- Medical expertise (CMO, advisors)
- Partnerships (Alpha-1 Foundation, medical societies)
- User research and feedback

**Activities**:

- Platform development and launch
- User verification and onboarding
- Content moderation
- AATD awareness campaign
- Research integration
- Community building

**Outputs**:

- 25,000-100,000 verified users (depending on scenario)
- 100,000+ clinical discussions
- 10,000+ research papers shared
- 50+ educational content pieces
- 50+ specialty communities

**Outcomes** (Short-term: 6-12 months):

- Increased AATD awareness among physicians
- Active, engaged user community
- High-quality clinical discussions
- Platform satisfaction (NPS >40)

**Outcomes** (Medium-term: 12-24 months):

- Earlier rare disease diagnoses documented
- Physician practice changes reported
- Reduced diagnostic delays measured
- Healthcare cost savings calculated

**Impact** (Long-term: 24+ months):

- Lives saved through earlier diagnosis
- Medical errors prevented
- Research-to-practice gap reduced
- Global medical knowledge democratized

---

## Primary Outcomes & Measurement Methods

### 1. Earlier Rare Disease Diagnosis (Primary Health Outcome)

**Outcome Statement**: Platform discussions lead to earlier diagnosis of AATD and other rare diseases, preventing irreversible complications.

#### Measurement Methods

**1.1 User-Reported Diagnostic Case Studies**

**Method**: Structured survey for physicians who made diagnoses influenced by platform

**Survey Questions**:

- What was the diagnosis?
- What platform discussion(s) influenced your thinking?
- How long had patient experienced symptoms before diagnosis?
- What would likely have happened without this platform insight? (delayed diagnosis, misdiagnosis, etc.)
- What complications were prevented?
- Please provide de-identified case details

**Data Collection**:

- In-app prompt: "Did a discussion on this platform help you diagnose a patient?"
- Monthly email to active users
- Incentive: $50 gift card for detailed case studies (20+ cases/year)

**Target**: 500+ documented cases Year 1, 5,000+ by Year 2

**Analysis**:

- Qualitative coding of case studies
- Calculate average diagnostic delay reduction
- Estimate complications prevented
- Peer review by Medical Advisory Board

---

**1.2 Partner Institution Diagnostic Rate Tracking**

**Method**: Collaborate with Alpha-1 Foundation and partner hospitals to track AATD testing rates

**Data Sources**:

- Alpha-1 Foundation diagnostic testing database
- Partner hospital electronic health record (EHR) data
- State health department rare disease registries

**Metrics**:

- Number of Alpha-1 tests ordered per month (pre vs. post campaign)
- Positive test rate (% of tests that confirm AATD)
- Time from symptom onset to diagnosis (patient-reported)
- Geographic distribution of new diagnoses

**Control Group**: Compare testing rates in:

- Hospitals where physicians use platform (intervention)
- Similar hospitals where physicians don't use platform (control)
- Same hospitals pre-launch (historical control)

**Statistical Analysis**:

- Interrupted time series analysis
- Difference-in-differences regression
- Control for confounders (conference attendance, other campaigns)

**Target**: 25% increase in AATD testing rates in partner institutions

**Sample Size**: 10-20 partner institutions (5,000-10,000 physicians)

---

**1.3 Patient-Reported Outcomes**

**Method**: Survey patients diagnosed with AATD or other rare diseases after platform launch

**Survey administered by**: Alpha-1 Foundation, NORD, disease-specific advocacy groups

**Survey Questions**:

- When were you diagnosed?
- How long did you experience symptoms before diagnosis?
- How many physicians did you see before diagnosis?
- Do you know if your diagnosing physician uses Uniting Doctors platform?
- What impact did earlier/later diagnosis have on your health?

**Target**: 100+ patient surveys per year

**Analysis**: Patient perspective on diagnostic journey, quality of life impact

---

### 2. Improved Medical Knowledge (Secondary Outcome)

**Outcome Statement**: Platform use increases physician awareness of rare diseases and current medical evidence, leading to practice changes.

#### Measurement Methods

**2.1 Knowledge Assessment Surveys**

**Pre-Post Design**:

- **Baseline survey**: New users complete upon registration
- **Follow-up survey**: Same users complete at 3, 6, 12 months

**Survey Content**:

- **AATD Knowledge** (10 questions):
  - "What is Alpha-1 Antitrypsin Deficiency?"
  - "What are the primary symptoms of AATD?"
  - "When should you order an Alpha-1 test?"
  - "What is the prevalence of AATD?"
  - etc.
- **General Rare Disease Knowledge** (10 questions)
- **Evidence-Based Practice** (10 questions on current guidelines)

**Scoring**: % correct, compared pre vs. post

**Target**:

- Baseline AATD knowledge: 23% correct (from pilot survey)
- 6-month follow-up: 50% correct (117% improvement)
- 12-month follow-up: 60% correct

**Sample Size**: 500+ users (power analysis: detect 20% difference with 80% power)

---

**2.2 Self-Reported Practice Changes**

**Method**: Quarterly survey of active users

**Questions**:

- "In the past 3 months, has a discussion on this platform changed your clinical practice?" (Yes/No)
- If yes: "Please describe the practice change"
- "How confident are you that this change improved patient care?" (1-5 scale)
- "Have you ordered a test you wouldn't have otherwise ordered?" (Yes/No)
- If yes: "What test, and what was the result?"

**Target**: 40% of users report practice change annually

**Analysis**: Qualitative coding of practice changes, categorize by type:

- Diagnostic testing
- Treatment changes
- Referral patterns
- Patient counseling
- Preventive care

---

**2.3 Research Engagement Metrics**

**Quantitative Metrics** (platform analytics):

- % of users who engage with research features monthly
- Number of papers saved per user per month
- Time spent reading papers on platform
- Papers cited in discussions (% of posts with citations)

**Target**:

- 30% of users engage with research monthly
- Average 2+ papers saved per active user per month
- 20% of posts include citations (evidence-based discussions)

---

### 3. Reduced Medical Errors (Tertiary Outcome)

**Outcome Statement**: Knowledge sharing prevents medical errors through collective learning from near-misses and mistakes.

#### Measurement Methods

**3.1 User-Reported Error Prevention**

**Method**: Anonymous survey (encourage honest reporting)

**Questions**:

- "Has a discussion on this platform helped you avoid a potential medical error?" (Yes/No)
- If yes: "What type of error?" (Diagnostic, medication, procedural, etc.)
- "How did you learn about this on the platform?"
- "What was the potential consequence if error had occurred?" (Minor, moderate, major harm)

**Target**: 1,000+ error prevention reports per year

**Extrapolation**: If 1,000 users report preventing 1 error each, and we have 25,000 users, potential 10,000+ errors prevented (assuming 40% response rate)

---

**3.2 Safety Discussion Analysis**

**Method**: Content analysis of platform discussions related to safety

**Identify posts/comments containing**:

- "Near miss"
- "Adverse event"
- "Medication error"
- "Diagnostic error"
- "Patient safety"

**Analyze**:

- Number of safety-related discussions
- Quality of discussions (actionable insights?)
- Community engagement (comments, upvotes)
- Evidence of learning (follow-up posts)

**Target**: 500+ safety discussions per year

---

### 4. Healthcare Cost Savings (Economic Outcome)

**Outcome Statement**: Earlier diagnosis and error prevention generate substantial healthcare cost savings.

#### Measurement Methods

**4.1 Cost Savings Model**

**Approach**: Conservative economic analysis with peer review

**AATD-Specific Calculations**:

- **Late diagnosis cost**:
  - Undiagnosed AATD patient → COPD treatment + eventual lung transplant
  - Annual COPD treatment: $10,000-30,000
  - Lung transplant: $500,000+
  - Total lifetime cost: $800,000 (average)

- **Early diagnosis cost**:
  - Augmentation therapy: $150,000/year
  - Prevent progression, avoid transplant
  - Total lifetime cost: $3,000,000 (therapy) - $500,000 (avoided transplant) = $2,500,000

- **Net cost**: Early diagnosis MORE expensive initially but improves quality of life and prevents death

**Better Metric: Complications Prevented**:

- Each early AATD diagnosis prevents:
  - 10+ years of progressive lung damage
  - Potential liver disease
  - 60% reduction in exacerbations
  - Quality-adjusted life years (QALYs) gained: 5-10

**Other Rare Diseases**: Apply similar methodology

**Medical Error Prevention Savings**:

- Average cost of medical error: $17,000 (medication error) to $1,000,000+ (surgical error)
- Conservative estimate: $50,000 average per prevented error
- 10,000 errors prevented × $50,000 = $500M savings

**Total Estimated Savings Year 1**: $100-250M

**Model Validation**:

- Peer review by health economists
- Sensitivity analysis (vary assumptions)
- Conservative estimates (prefer underestimate to overestimate)

---

## Platform Success Metrics

### 1. User Engagement Metrics

**Daily Active Users (DAU) / Monthly Active Users (MAU)**:

- Target: 20-30% DAU/MAU ratio
- Industry benchmark: 15-25% (social platforms)
- Measured continuously via platform analytics

**Retention Rates**:

- 7-day retention: % of users who return within 7 days
  - Target: ≥40%
- 30-day retention: % of users who return within 30 days
  - Target: ≥30%
- Cohort analysis: Track retention by signup month

**Session Metrics**:

- Average session duration: Target 10-15 minutes
- Sessions per week: Target 2-3 for active users
- Pages per session: Target 5-10

**Content Creation**:

- Posts per day: Target 50 (Month 6) → 200 (Month 12) → 500 (Month 24)
- Comments per post: Target average 5+
- Upvotes per post: Target average 10+

**Community Health**:

- % of questions that receive answers: Target 75%+
- Time to first response: Target <4 hours median
- % of users who have posted: Target 30%+
- % of users who have commented: Target 60%+

---

### 2. User Satisfaction Metrics

**Net Promoter Score (NPS)**:

- Question: "How likely are you to recommend Uniting Doctors to a colleague?" (0-10)
- Calculation: % Promoters (9-10) - % Detractors (0-6)
- Target: NPS >40 (excellent for B2B/professional platforms)
- Measured quarterly

**Platform Usability (SUS - System Usability Scale)**:

- 10-question standardized survey
- Score 0-100
- Target: >80 (excellent usability)
- Measured at 6, 12, 24 months

**Feature Satisfaction**:

- Rate satisfaction with specific features (1-5 scale)
- Identify features users love vs. features needing improvement
- Measured quarterly

**Qualitative Feedback**:

- Open-ended survey questions
- User interviews (10-15 per quarter)
- Analyze themes: What's working? What's frustrating? What's missing?

---

### 3. Community Quality Metrics

**Evidence-Based Discussions**:

- % of posts that cite research: Target 20%+
- % of posts tagged with evidence level: Target 50%+
- Quality score (based on upvotes, comments, saves)

**Moderation Metrics**:

- Content flagged for moderation: Track volume and trends
- False positive rate: Target <5%
- Average moderation response time: Target <2 hours
- User appeals: Track rate and outcomes

**Toxicity/Safety**:

- Zero tolerance for HIPAA violations
- Toxicity score (Perspective API): Target <5% of content flagged
- User reports of harassment: Track and respond quickly

---

## Data Collection Methods Summary

### Quantitative Data Sources

**1. Platform Analytics** (Continuous, Automated)

- User registration and demographics
- Login frequency and session duration
- Posts, comments, votes (volume and timing)
- Feature usage (search, save, share, etc.)
- Retention and churn

**Tools**: Custom analytics dashboard (PostgreSQL queries), Google Analytics, Mixpanel

---

**2. Surveys** (Periodic)

- **Onboarding survey**: All new users (demographics, expectations, baseline knowledge)
- **Pulse surveys**: Monthly, 2-3 questions (satisfaction, recent value)
- **Comprehensive surveys**: Quarterly, 15-20 questions (NPS, usability, knowledge, practice changes)
- **Case study surveys**: As-needed, detailed diagnostic case collection

**Tools**: Typeform, SurveyMonkey, in-platform survey widget

**Response Rate Targets**:

- Onboarding: 80%+ (required for account completion)
- Pulse: 20-30%
- Comprehensive: 15-25%
- Case study: 5-10% (with incentives)

---

**3. Partner Institution Data** (Periodic, Collaborative)

- EHR data on testing rates (Alpha-1, other rare disease tests)
- De-identified patient outcomes
- Diagnostic delay metrics

**Agreements**: Data sharing agreements, IRB approval where needed

---

### Qualitative Data Sources

**1. User Interviews** (Ongoing)

- 10-15 interviews per quarter
- 30-45 minutes each
- Rotating specialties, career stages, geographies
- Semi-structured protocol (prepared questions + open discussion)

**Topics**:

- Platform experience (what works, what doesn't)
- Clinical value (how has it helped your practice?)
- Specific feature feedback
- Ideas for improvement

**Compensation**: $100 gift card

---

**2. Case Studies** (Ongoing)

- Detailed narratives of earlier diagnoses
- Practice change stories
- Error prevention examples
- Community impact stories

**Collection**:

- In-app prompts
- Outreach to users with high engagement
- Partnership with patient advocacy groups

**Target**: 50+ detailed case studies per year

---

**3. Focus Groups** (Annual)

- 6-8 users per group, 90 minutes
- Virtual (Zoom)
- Specialty-specific groups (pulmonologists, nurses, etc.)
- Geographic groups (rural physicians, international users)

**Target**: 4-6 focus groups per year

---

**4. Content Analysis** (Periodic)

- Qualitative coding of discussions
- Themes: Types of questions, knowledge gaps, community norms
- Identify high-impact discussions (exemplars)

**Method**: Random sample of 500 posts per quarter, qualitative analysis software (NVivo)

---

## External Evaluation Partnership

### Role of External Evaluator

**Independence**: Third-party academic institution or research firm

**Responsibilities**:

1. **Evaluation Design**: Review and refine evaluation framework
2. **Data Collection**: Design surveys, interview protocols
3. **Analysis**: Statistical analysis, qualitative coding
4. **Reporting**: Quarterly progress reports, annual comprehensive report
5. **Publication**: Co-author peer-reviewed papers

**Qualifications**:

- PhD in public health, health services research, or related field
- Experience evaluating digital health interventions
- Mixed-methods research expertise
- Publication track record

**Budget**: $30K-80K depending on grant scenario

**Timeline**:

- Month 1-2: Finalize evaluation plan
- Months 3-24: Ongoing data collection and analysis
- Month 24+: Final report and publications

---

## Evaluation Timeline

### Year 1 (Months 1-12)

**Month 1-3: Evaluation Setup**

- Partner with external evaluator
- Finalize evaluation framework
- Design surveys and data collection instruments
- Obtain IRB approval (if needed)
- Establish data sharing agreements with partners
- Create analytics dashboard

**Month 3-6: Baseline & Beta Data**

- Collect baseline data (beta users)
- Onboarding surveys (demographics, knowledge pre-test)
- Usability testing
- First pulse surveys

**Month 6-9: Public Launch Data**

- Continued onboarding surveys (growing user base)
- Monthly pulse surveys
- First comprehensive survey (Month 6)
- Begin case study collection
- User interviews (10-15)

**Month 9-12: Year 1 Impact Assessment**

- Knowledge post-tests (3-month, 6-month cohorts)
- Practice change surveys
- Comprehensive quarterly survey
- Partner institution data analysis (AATD testing rates)
- Year 1 evaluation report (external evaluator)

---

### Year 2 (Months 13-24)

**Ongoing Data Collection**:

- Monthly pulse surveys
- Quarterly comprehensive surveys
- Continuous analytics tracking
- User interviews (40-60 total Year 2)
- Case study collection (50+ cases)
- Focus groups (4-6)

**Analysis & Reporting**:

- Mid-Year 2 progress report (Month 18)
- Partner institution analysis (diagnostic rates, outcomes)
- Cost savings modeling (health economist collaboration)
- Peer-reviewed publication drafting (3-5 papers)

**Final Evaluation** (Month 24):

- Comprehensive 24-month evaluation report
- Multi-site impact study (if Scenario C)
- Publication preparation
- Lessons learned and recommendations

---

## Dissemination Plan

### Target Audiences

1. **Funders**: Quarterly reports, annual comprehensive reports
2. **Medical Community**: Peer-reviewed journals, conference presentations
3. **Patient Advocacy**: Partnership updates, patient impact stories
4. **Public**: Blog posts, media coverage, annual report

### Publications (Target: 5-10 papers over 24 months)

**Paper 1**: "Design and Implementation of a Digital Platform for Medical Knowledge Sharing" (Month 12)

- **Journal**: JMIR (Journal of Medical Internet Research)
- **Content**: Platform design, user engagement metrics, early outcomes

**Paper 2**: "Impact of Online Community on AATD Diagnostic Rates: A Multi-Site Study" (Month 18)

- **Journal**: Orphanet Journal of Rare Diseases or Chest
- **Content**: Diagnostic rate analysis, case studies, health outcomes

**Paper 3**: "Physician Knowledge and Practice Changes from Online Professional Community" (Month 24)

- **Journal**: Academic Medicine or Medical Education
- **Content**: Knowledge assessment results, practice change survey, educational implications

**Paper 4**: "Healthcare Cost Savings from Earlier Rare Disease Diagnosis" (Month 24)

- **Journal**: Value in Health or Health Affairs
- **Content**: Economic analysis, cost savings model, policy implications

**Paper 5**: "Patterns of Medical Knowledge Sharing in Online Communities" (Month 24)

- **Journal**: JAMIA (Journal of the American Medical Informatics Association)
- **Content**: Content analysis, community dynamics, design implications

**Additional Papers** (if Scenario C):

- Multi-center impact study
- Patient-reported outcomes
- Mobile app effectiveness
- AI-powered features evaluation
- Global health equity impact

---

### Conference Presentations (Target: 5-15 over 24 months)

**Medical Conferences**:

- American Thoracic Society (ATS) Annual Meeting
- American Medical Association (AMA) Annual Meeting
- American College of Physicians (ACP) Internal Medicine
- Rare Disease Day events

**Health IT Conferences**:

- HIMSS (Healthcare Information and Management Systems Society)
- AMIA (American Medical Informatics Association)
- Connected Health Conference

**Medical Education Conferences**:

- AAMC (Association of American Medical Colleges)
- AMSA (American Medical Student Association)

---

### Annual Public Impact Report

**Content**:

- Executive summary (1-2 pages)
- User growth and engagement
- Health impact stories (case studies)
- Research findings (preliminary and published)
- Financial summary
- Year ahead goals

**Distribution**:

- Website (public, downloadable PDF)
- Email to users, partners, funders
- Social media campaign
- Media outreach

---

## Evaluation Quality Assurance

### Ensuring Rigor and Credibility

**1. External Evaluator**: Independent, experienced researcher

**2. Comparison Groups**: Control groups where possible (pre-post, intervention vs. control institutions)

**3. Multiple Data Sources**: Triangulation (surveys + interviews + analytics + partner data)

**4. Validated Instruments**: Use established scales (NPS, SUS, etc.) where available

**5. Statistical Rigor**:

- Power analysis (adequate sample sizes)
- Control for confounders
- Sensitivity analysis
- Conservative assumptions

**6. Qualitative Rigor**:

- Systematic coding (intercoder reliability)
- Member checking (verify interpretations with participants)
- Thick description (detailed case studies)

**7. Peer Review**: Medical Advisory Board reviews findings

**8. Transparency**:

- Pre-register evaluation plan
- Publish null findings (not just positive results)
- Open data (de-identified) where possible

---

## Limitations and Mitigation

### Known Limitations

**1. Self-Selection Bias**: Users who join platform may differ from non-users

- **Mitigation**: Recruit diverse users, compare to population demographics

**2. Self-Report Bias**: Users may overreport practice changes or impact

- **Mitigation**: Triangulate with objective data (partner institution records), anonymous surveys

**3. Attribution Challenge**: Hard to prove platform caused diagnostic change

- **Mitigation**: Detailed case studies with causal chain, comparison groups

**4. Recall Bias**: Users may not remember platform's influence

- **Mitigation**: Collect data in real-time (pulse surveys), in-app prompts

**5. Hawthorne Effect**: Users may change behavior because they're being studied

- **Mitigation**: Acknowledge limitation, compare to organic usage patterns

### Addressing Limitations

- **Honest reporting**: Acknowledge limitations in publications
- **Conservative estimates**: Prefer underestimating impact to overstating
- **Multiple perspectives**: Combine physician, patient, and institutional data

---

## Conclusion

Our evaluation framework is comprehensive, rigorous, and designed to demonstrate real health impact. We will measure what matters: **lives saved, knowledge gained, errors prevented, and costs reduced**.

Through partnership with an independent external evaluator, multiple data sources, and transparent reporting, we will provide credible evidence of the platform's effectiveness and contribute to medical knowledge through peer-reviewed publications.

**We don't just build a platform—we prove it works.**

---

[← Previous: Methodology](./03_METHODOLOGY_IMPLEMENTATION.md) | [Back to Index](./00_GRANT_PROPOSAL_INDEX.md) | [Next: Social Impact →](./05_SOCIAL_IMPACT_EQUITY.md)

**Questions about our evaluation plan?**
grants@unitingdoctors.org
